---
title: 基于深度强化学习的汽车跟踪控制纵向动力学与运动学模型
date: 2019-07-26 22:25:55
tags: [学术,文献阅读,强化学习,纵向运动]
category: [学术,文献阅读,强化学习,纵向运动]
---

# 基于深度强化学习的汽车跟踪控制纵向动力学与运动学模型

[^1]: Lin Y, McPhee J, Azad N L. Longitudinal Dynamic versus Kinematic Models for Car-following Control Using Deep Reinforcement Learning[J]. arXiv preprint arXiv:1905.08314, 2019.

# 作者信息

| 作者           | 学校/机构                |
| -------------- | ------------------------ |
| Yuan Lin       | 滑铁卢大学系统设计工程系 |
| John McPhee    | 滑铁卢大学系统设计工程系 |
| Nasser L. Azad | 滑铁卢大学系统设计工程系 |

## 摘要

目前基于深度强化学习(DRL)的自主车辆控制研究大多采用质点运动学模型，忽略了包括加速延迟和加速指令动力学在内的车辆动力学。加速度延迟是由传感和驱动延迟引起的，导致控制输入的执行延迟。加速度指令动力学表明，由于摩擦和路面等级的原因，车辆的实际加速度不会立即达到所需的指令加速度。在此工作中，我们研究了将使用车辆运动学模型训练的DRL控制器应用于更真实的车辆动力学驱动控制的可行性。我们考虑一个特殊的纵向车辆跟踪控制，即，自适应巡航控制，采用点质量运动学模型通过DRL求解。当将该控制器应用于具有车辆动力学特性的车辆跟踪时，我们发现车辆跟踪性能显著下降。因此，我们重新设计了DRL框架，通过将延迟控制输入和实际车辆加速度添加到强化学习环境状态，来适应加速度延迟和加速度命令动力学。训练结果表明，与动态规划方法相比，在考虑车辆动力学的情况下，重新设计的DRL控制器具有接近最优的车辆跟随控制性能。

## 主要内容

设计了一种考虑真实车身动力学和执行机构时延的DRL控制算法，实现自动驾驶纵向控制。

## 解决的问题

以前的算法在设计DRL控制算法时没有考虑执行机构的时延，本文的算法考虑时延因素对算法做了修改，提升了算法性能。

## 使用的方法

将延时和未执行的指令作为强化学习环境状态(state)的一部分

# 结论

通过使用DRL（深度强化学习）解决特定的跟车控制问题，我们证明了使用质点运动模型训练的DRL控制器无法推广以解决具有车辆加速延迟和命令动态的更真实的控制情况。我们将延迟和未执行的控制输入以及车辆的实际加速度作为具有车辆动力学模型的DRL控制器的环境状态(state)。训练结果表明，该方法为车辆动力学跟车控制提供了近乎最优的解决方案。

当把延迟的控制添加到强化学习环境状态中时，希望DRL算法可以使用延迟控制输入预测未来的系统行为并选择下一个最优动作。我们的结果表明，DRL代理能够在训练后以接近最佳的方式进行训练。然而，由于环境状态随着更多变量而增加，因此需要增加神经网络的大小和更长的训练时间，这是一个不利因素。如引言中所述，另一种方法是分别学习基础动态系统，并使用学习系统预测未来延迟时间后的系统行为，以确定当前的控制动作[27]。然而，该方法对于挑战自动驾驶控制系统（例如合并控制）可能是不可行的，因为这样的系统由于多车辆交互而经受许多变化和干扰。为这样的系统开发或学习准确的模型可能并不容易。

# 展望

未来的工作包括开发一种更强大的自行车DRL控制器，可以通过前车速度的丰富变化进行训练。另一个研究方向是开发具有车辆动力学的DRL控制器，以考虑更具挑战性的自动驾驶场景，例如高速公路入口坡道合并。

## 特点

***在算法设计中考虑车辆动力学，而不是将车辆当作一个质点。***



## 参考文献

1. Lin Y, McPhee J, Azad N L. Longitudinal Dynamic versus Kinematic Models for Car-following Control Using Deep Reinforcement Learning[J]. arXiv preprint arXiv:1905.08314, 2019.